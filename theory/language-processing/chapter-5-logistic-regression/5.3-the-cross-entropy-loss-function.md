# 5.3 The Cross-entropy Loss Function

* A loss function is required to determine how close the classifier output is to the gold output.
* TODO consider moving this part to the [Information Theory ](../../information-theory/)section.
* **DEF cross-entropy & Kullback-Leibler divergence / relative entropy**  给定同一事件空间上的两个分布 p 和 q，假设我们认为 p 是事件的真实分布，q 是一个预测分布。我们知道如果根据 p 设计事件的理想编码，其平均码长将为熵 H\(p\)；但如果根据 q 设计事件的“理想编码”，得到的编码将不是事件的理想编码，因此平均码长将不小于 H\(p\)。这一平均码长就是交叉熵，记作 $$H(p,q)$$；它比理想码长多使用的比特数就是 KL 散度，或叫相对熵，记作 $$D_{KL}(p || q)$$ 。
  * 注意这里交叉熵的记号和联合熵的记号是一样的，但并不是同一个概念。
* 从直觉上来说，考虑 $$H(p) = -\sum p(x) \log p(x) = E_p (-\log p(x))$$ ，因为它同时是理想编码码长的期望值，因此可以认为 $$-log p(x)$$ 是发生概率为 p 的事件 x 的对应理想码长；那么如果根据 q 设计事件的“理想编码”，得到的事件 x 的码长将为 $$-\log q(x)$$ ，因此平均码长为 $$-\sum p(x) \log q(x)$$ ，这就是交叉熵的表达式；使用的多余比特数为 $$-\sum p(x) \log q(x) + \sum p(x) \log p(x) = \sum p(x) \log \frac{p(x)}{q(x)}$$ ，这就是 KL 散度的表达式。
* 互信息 $$I(X;Y) = \sum p(x,y) \log \frac{p(x,y)}{p(x)p(y)}$$ 可以理解为 KL 散度的特殊情形，即 $$I(X;Y) = D_{KL}(P_{(x,y)}||P_X \otimes P_Y )$$ 。
* 我们可以使用交叉熵函数来衡量预测分布到真实分布的距离。对于分类问题，classifier output 给出被分类对象归属于各个类别的概率，也就是给出了一个在各个类别上的概率分布，这就是预测分布；而真实分布来自 gold output，被分类对象必属于类别之一，也就是真实分布为该对象分属这一类别的概率为 1，分属其余类别的概率为 0。我们可以使用交叉熵函数衡量二者的距离，并将结果作为损失函数。
  * 虽然我们说它衡量分布到分布的距离，但要注意它并不符合严格的距离的性质。它不具有对称性和矢量三角形性质。

## Reference

\[1\] [https://en.wikipedia.org/wiki/Cross\_entropy](https://en.wikipedia.org/wiki/Cross_entropy)

